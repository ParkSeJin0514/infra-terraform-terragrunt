# ============================================================================
# .github/workflows/terraform-all-destroy.yml
# ============================================================================
# Terraform/Terragrunt ì¸í”„ë¼ ì‚­ì œ ì›Œí¬í”Œë¡œìš°
# - EKS ë¦¬ì†ŒìŠ¤ ì‚¬ì „ ì •ë¦¬ (ArgoCD, Karpenter, Ingress, CRD ë“±)
# - ALB ëª…ì‹œì  ì‚­ì œ (argocd-alb, petclinic ê´€ë ¨)
# - Terragrunt destroy ì‹¤í–‰
#
# ê°œì„ ì‚¬í•­:
# - Karpenter ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ë¡œì§ ì¶”ê°€ (NodeClaim, NodePool, EC2NodeClass)
# - Karpenterê°€ í”„ë¡œë¹„ì €ë‹í•œ EC2 ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ
# - CRD Finalizer ì œê±° ë¡œì§ ì¶”ê°€
# - Terraform ë²„ì „ 1.9.0 ì—…ê·¸ë ˆì´ë“œ
# - -refresh=false ì˜µì…˜ ì¶”ê°€ (state ê¼¬ì„ ë°©ì§€)
# - TERRAGRUNT_IGNORE_DEPENDENCY_ERRORS ì¶”ê°€
# - ë°±ê·¸ë¼ìš´ë“œ ì‚­ì œë¡œ íƒ€ì„ì•„ì›ƒ ë°©ì§€
# - ALB ëª…ì‹œì  ì‚­ì œ ë¡œì§ ì¶”ê°€ (argocd-alb ë“±)
# - for ë£¨í”„ ê¸°ë°˜ ëª…ì‹œì  finalizer ì œê±° ì¶”ê°€
# - ArgoCD ì „ì²´ ì»´í¬ë„ŒíŠ¸ ì¤‘ì§€ ë¡œì§ ì¶”ê°€ (finalizer ì¬ì¶”ê°€ ë°©ì§€)
# ============================================================================

name: 'Terraform Destroy'

on:
  workflow_dispatch:
    inputs:
      confirm:
        description: 'ì‚­ì œ í™•ì¸ (destroy ì…ë ¥)'
        required: true
        default: ''
      layer:
        description: 'ì‚­ì œ ë ˆì´ì–´ ì„ íƒ'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - bootstrap
          - compute
          - foundation

env:
  AWS_REGION: ap-northeast-2
  TF_VERSION: '1.9.0'
  TG_VERSION: '0.54.0'
  TERRAGRUNT_IGNORE_DEPENDENCY_ERRORS: "true"

permissions:
  id-token: write
  contents: read

jobs:
  # ============================================================================
  # í™•ì¸ ë‹¨ê³„
  # ============================================================================
  confirm:
    name: 'Confirm Destroy'
    runs-on: ubuntu-latest
    steps:
      - name: Check confirmation
        if: github.event.inputs.confirm != 'destroy'
        run: |
          echo "âŒ ì‚­ì œë¥¼ ì§„í–‰í•˜ë ¤ë©´ 'destroy'ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
          exit 1
      
      - name: Confirmed
        run: echo "âœ… ì‚­ì œê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤."

  # ============================================================================
  # EKS ë¦¬ì†ŒìŠ¤ ì‚¬ì „ ì •ë¦¬
  # ============================================================================
  pre-cleanup:
    name: 'Pre-Cleanup EKS Resources'
    needs: confirm
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install tools
        run: |
          # kubectl ì„¤ì¹˜
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          # jq ì„¤ì¹˜ (finalizer ì œê±°ìš©)
          sudo apt-get update && sudo apt-get install -y jq

      - name: Check EKS Cluster
        id: check-eks
        run: |
          CLUSTER_NAME=$(aws eks list-clusters --query 'clusters[0]' --output text 2>/dev/null || echo "")
          if [ -n "$CLUSTER_NAME" ] && [ "$CLUSTER_NAME" != "None" ]; then
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
            echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
            echo "âœ… EKS í´ëŸ¬ìŠ¤í„° ë°œê²¬: $CLUSTER_NAME"
          else
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
            echo "â„¹ï¸ EKS í´ëŸ¬ìŠ¤í„° ì—†ìŒ - ì •ë¦¬ ìŠ¤í‚µ"
          fi

      - name: Update kubeconfig
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          aws eks update-kubeconfig --name ${{ steps.check-eks.outputs.cluster_name }} --region ${{ env.AWS_REGION }}

      # ========================================================================
      # ArgoCD ì»´í¬ë„ŒíŠ¸ ì¤‘ì§€ (ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
      # ========================================================================
      - name: Stop ArgoCD Components
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=== ArgoCD ì „ì²´ ì»´í¬ë„ŒíŠ¸ ì¤‘ì§€ (finalizer ì¬ì¶”ê°€ ë°©ì§€) ==="
          
          # ëª¨ë“  ArgoCD Deployment ì¤‘ì§€
          kubectl scale deploy --all -n argocd --replicas=0 2>/dev/null || true
          
          # ëª¨ë“  ArgoCD StatefulSet ì¤‘ì§€
          kubectl scale statefulset --all -n argocd --replicas=0 2>/dev/null || true
          
          # ê°œë³„ ì»´í¬ë„ŒíŠ¸ ëª…ì‹œì  ì¤‘ì§€ (ë°±ì—…)
          kubectl scale deploy argocd-application-controller -n argocd --replicas=0 2>/dev/null || true
          kubectl scale deploy argocd-server -n argocd --replicas=0 2>/dev/null || true
          kubectl scale deploy argocd-repo-server -n argocd --replicas=0 2>/dev/null || true
          kubectl scale deploy argocd-dex-server -n argocd --replicas=0 2>/dev/null || true
          kubectl scale deploy argocd-redis -n argocd --replicas=0 2>/dev/null || true
          kubectl scale deploy argocd-notifications-controller -n argocd --replicas=0 2>/dev/null || true
          kubectl scale statefulset argocd-application-controller -n argocd --replicas=0 2>/dev/null || true
          
          echo "=== ArgoCD Pod ì¢…ë£Œ ëŒ€ê¸° (10ì´ˆ) ==="
          sleep 10
          
          # Pod ê°•ì œ ì‚­ì œ
          kubectl delete pods --all -n argocd --force --grace-period=0 2>/dev/null || true
          
          echo "âœ… ArgoCD ì»´í¬ë„ŒíŠ¸ ì¤‘ì§€ ì™„ë£Œ"

      # ========================================================================
      # ArgoCD ì •ë¦¬ (Finalizer ì œê±° í¬í•¨)
      # ========================================================================
      - name: Cleanup ArgoCD Applications (Batch Finalizer Removal)
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=== ArgoCD Application Finalizer ì¼ê´„ ì œê±° ==="
          
          # ë°©ë²• 1: for ë£¨í”„ë¡œ ëª…ì‹œì  ì²˜ë¦¬ (íŠ¹ì • ì•± ëŒ€ìƒ) - Karpenter ì¶”ê°€
          for app in alb-controller argocd-ingress efs-csi-driver external-secrets petclinic root-app karpenter karpenter-config; do
            kubectl patch application $app -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
          done
          
          # ë°©ë²• 2: ë™ì ìœ¼ë¡œ ë‚¨ì€ ì•± ì²˜ë¦¬
          for app in $(kubectl get applications -n argocd -o name 2>/dev/null); do
            kubectl patch $app -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
          done
          
          # ì‚­ì œ
          kubectl delete applications.argoproj.io --all -n argocd --force --grace-period=0 2>/dev/null || true
          
          # ì‚­ì œ í™•ì¸
          echo "=== Application ì‚­ì œ í™•ì¸ ==="
          kubectl get applications -n argocd 2>/dev/null || echo "No applications found"
          
          echo "=== ArgoCD ApplicationSet Finalizer ì¼ê´„ ì œê±° ==="
          for appset in $(kubectl get applicationsets -n argocd -o name 2>/dev/null); do
            kubectl patch $appset -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
          done
          kubectl delete applicationsets.argoproj.io --all -n argocd --force --grace-period=0 2>/dev/null || true
          
          echo "=== ArgoCD AppProject Finalizer ì¼ê´„ ì œê±° ==="
          for proj in $(kubectl get appprojects -n argocd -o name 2>/dev/null); do
            kubectl patch $proj -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
          done
          kubectl delete appprojects.argoproj.io --all -n argocd --force --grace-period=0 2>/dev/null || true
          
          echo "âœ… ArgoCD ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ"

      - name: Delete ArgoCD CRDs with Finalizer removal
        if: steps.check-eks.outputs.cluster_exists == 'true'
        timeout-minutes: 3
        run: |
          echo "=== ArgoCD CRD Finalizer ì œê±° ë° ì‚­ì œ ==="
          for CRD in applications.argoproj.io applicationsets.argoproj.io appprojects.argoproj.io; do
            if kubectl get crd $CRD > /dev/null 2>&1; then
              echo "Processing CRD: $CRD"
              kubectl patch crd $CRD -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
              kubectl delete crd $CRD --force --grace-period=0 &
            fi
          done
          sleep 10
          echo "CRD ì‚­ì œ ìš”ì²­ ì™„ë£Œ"

      # ========================================================================
      # External Secrets ì •ë¦¬
      # ========================================================================
      - name: Cleanup External Secrets (Batch Finalizer Removal)
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=== External Secrets Controller ì¤‘ì§€ ==="
          kubectl scale deploy --all -n external-secrets --replicas=0 2>/dev/null || true
          kubectl delete pods --all -n external-secrets --force --grace-period=0 2>/dev/null || true
          sleep 5
          
          echo "=== ClusterSecretStore Finalizer ì¼ê´„ ì œê±° ==="
          for css in $(kubectl get clustersecretstore -o name 2>/dev/null); do
            kubectl patch $css -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
          done
          kubectl delete clustersecretstore --all --force --grace-period=0 2>/dev/null || true
          
          echo "=== SecretStore Finalizer ì¼ê´„ ì œê±° (ëª¨ë“  ë„¤ì„ìŠ¤í˜ì´ìŠ¤) ==="
          for ss in $(kubectl get secretstore -A -o name 2>/dev/null); do
            kubectl patch $ss -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
          done
          kubectl delete secretstore --all -A --force --grace-period=0 2>/dev/null || true
          
          echo "=== ExternalSecret ì‚­ì œ ==="
          kubectl delete externalsecret --all -A --ignore-not-found=true 2>/dev/null || true
          
          echo "=== External Secrets CRD Finalizer ì œê±° ë° ì‚­ì œ ==="
          for CRD in externalsecrets.external-secrets.io clustersecretstores.external-secrets.io secretstores.external-secrets.io clusterexternalsecrets.external-secrets.io; do
            if kubectl get crd $CRD > /dev/null 2>&1; then
              kubectl patch crd $CRD -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
              kubectl delete crd $CRD --force --grace-period=0 &
            fi
          done
          sleep 5
          echo "âœ… External Secrets ì •ë¦¬ ì™„ë£Œ"

      # ========================================================================
      # Karpenter ì •ë¦¬
      # ========================================================================
      - name: Cleanup Karpenter Resources
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=== Karpenter Controller ì¤‘ì§€ ==="
          kubectl scale deploy karpenter -n kube-system --replicas=0 2>/dev/null || true
          kubectl delete pods -l app.kubernetes.io/name=karpenter -n kube-system --force --grace-period=0 2>/dev/null || true
          sleep 5
          
          echo "=== Karpenter NodeClaim ì‚­ì œ (í”„ë¡œë¹„ì €ë‹ëœ ë…¸ë“œ ì¢…ë£Œ) ==="
          # NodeClaim Finalizer ì œê±° í›„ ì‚­ì œ
          for nc in $(kubectl get nodeclaims -o name 2>/dev/null); do
            echo "Processing NodeClaim: $nc"
            kubectl patch $nc -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
          done
          kubectl delete nodeclaims --all --force --grace-period=0 2>/dev/null || true
          
          echo "=== Karpenter NodePool Finalizer ì œê±° ë° ì‚­ì œ ==="
          for np in $(kubectl get nodepools -o name 2>/dev/null); do
            echo "Processing NodePool: $np"
            kubectl patch $np -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
          done
          kubectl delete nodepools --all --force --grace-period=0 2>/dev/null || true
          
          echo "=== Karpenter EC2NodeClass Finalizer ì œê±° ë° ì‚­ì œ ==="
          for ec2nc in $(kubectl get ec2nodeclasses -o name 2>/dev/null); do
            echo "Processing EC2NodeClass: $ec2nc"
            kubectl patch $ec2nc -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
          done
          kubectl delete ec2nodeclasses --all --force --grace-period=0 2>/dev/null || true
          
          echo "=== Karpenter CRD Finalizer ì œê±° ë° ì‚­ì œ ==="
          for CRD in nodepools.karpenter.sh nodeclaims.karpenter.sh ec2nodeclasses.karpenter.k8s.aws; do
            if kubectl get crd $CRD > /dev/null 2>&1; then
              echo "Processing CRD: $CRD"
              kubectl patch crd $CRD -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
              kubectl delete crd $CRD --force --grace-period=0 &
            fi
          done
          sleep 5
          
          echo "=== Karpenterê°€ í”„ë¡œë¹„ì €ë‹í•œ EC2 ì¸ìŠ¤í„´ìŠ¤ ì •ë¦¬ ==="
          # karpenter.sh/nodepool íƒœê·¸ê°€ ìˆëŠ” ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ
          KARPENTER_INSTANCES=$(aws ec2 describe-instances \
            --filters "Name=tag-key,Values=karpenter.sh/nodepool" "Name=instance-state-name,Values=running,pending" \
            --query 'Reservations[*].Instances[*].InstanceId' \
            --output text 2>/dev/null || true)
          
          if [ -n "$KARPENTER_INSTANCES" ]; then
            echo "Terminating Karpenter instances: $KARPENTER_INSTANCES"
            aws ec2 terminate-instances --instance-ids $KARPENTER_INSTANCES 2>/dev/null || true
            echo "Waiting for instances to terminate..."
            sleep 30
          else
            echo "No Karpenter instances found"
          fi
          
          echo "âœ… Karpenter ì •ë¦¬ ì™„ë£Œ"

      # ========================================================================
      # Ingress ë° ALB ì •ë¦¬
      # ========================================================================
      - name: Delete Ingresses (Batch Finalizer Removal)
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=== Ingress Finalizer ì¼ê´„ ì œê±° (ëª¨ë“  ë„¤ì„ìŠ¤í˜ì´ìŠ¤) ==="
          
          # ë°©ë²• 1: argocd-server ingress ëª…ì‹œì  ì²˜ë¦¬
          kubectl patch ingress argocd-server -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
          
          # ë°©ë²• 2: ë™ì ìœ¼ë¡œ ëª¨ë“  ingress ì²˜ë¦¬
          for ing in $(kubectl get ingress -A -o name 2>/dev/null); do
            kubectl patch $ing -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
          done
          
          echo "=== Ingress ì‚­ì œ (ALB íŠ¸ë¦¬ê±°) ==="
          for NS in petclinic monitoring argocd external-secrets kube-system default; do
            if kubectl get namespace $NS > /dev/null 2>&1; then
              kubectl delete ingress --all -n $NS --force --grace-period=0 2>/dev/null || true
            fi
          done
          echo "âœ… Ingress ì‚­ì œ ì™„ë£Œ"

      # ========================================================================
      # ALB ëª…ì‹œì  ì‚­ì œ
      # ========================================================================
      - name: Delete ALBs Explicitly
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=== ALB ëª…ì‹œì  ì‚­ì œ ==="
          
          ALB_NAMES=("argocd-alb" "petclinic-microservices-alb" "petclinic-monitoring-alb")
          
          for ALB_NAME in "${ALB_NAMES[@]}"; do
            echo "Checking ALB: $ALB_NAME"
            ALB_ARN=$(aws elbv2 describe-load-balancers --names "$ALB_NAME" \
              --query "LoadBalancers[0].LoadBalancerArn" --output text 2>/dev/null || echo "None")
            
            if [ -n "$ALB_ARN" ] && [ "$ALB_ARN" != "None" ]; then
              echo "Deleting ALB: $ALB_NAME (ARN: $ALB_ARN)"
              
              LISTENERS=$(aws elbv2 describe-listeners --load-balancer-arn "$ALB_ARN" \
                --query "Listeners[*].ListenerArn" --output text 2>/dev/null || true)
              for LISTENER in $LISTENERS; do
                echo "  Deleting Listener: $LISTENER"
                aws elbv2 delete-listener --listener-arn "$LISTENER" 2>/dev/null || true
              done
              
              aws elbv2 delete-load-balancer --load-balancer-arn "$ALB_ARN" 2>/dev/null || true
              echo "âœ… Deleted: $ALB_NAME"
            else
              echo "â„¹ï¸ ALB not found: $ALB_NAME"
            fi
          done
          
          echo ""
          echo "=== ALB ì‚­ì œ ëŒ€ê¸° (60ì´ˆ) ==="
          sleep 60

      - name: Delete Target Groups
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=== Target Group ì‚­ì œ ==="
          aws elbv2 describe-target-groups --query 'TargetGroups[*].[TargetGroupArn,TargetGroupName]' --output text 2>/dev/null | \
          while IFS=$'\t' read -r TG_ARN TG_NAME; do
            if [ -n "$TG_ARN" ] && [ "$TG_ARN" != "None" ]; then
              if [[ "$TG_NAME" == *petclinic* ]] || [[ "$TG_NAME" == *argocd* ]] || [[ "$TG_NAME" == k8s-* ]] || [[ "$TG_NAME" == *monitoring* ]]; then
                echo "Deleting TG: $TG_NAME"
                aws elbv2 delete-target-group --target-group-arn "$TG_ARN" 2>/dev/null || true
              fi
            fi
          done
          echo "âœ… Target Group ì‚­ì œ ì™„ë£Œ"

      # ========================================================================
      # Namespace ì •ë¦¬
      # ========================================================================
      - name: Cleanup Namespace Resources
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=== Namespace ë¦¬ì†ŒìŠ¤ ì‚­ì œ ==="
          for NS in petclinic monitoring argocd external-secrets; do
            if kubectl get namespace $NS > /dev/null 2>&1; then
              echo "Cleaning: $NS"
              kubectl delete deploy --all -n $NS --force --grace-period=0 2>/dev/null || true
              kubectl delete svc --all -n $NS --force --grace-period=0 2>/dev/null || true
              kubectl delete statefulset --all -n $NS --force --grace-period=0 2>/dev/null || true
              kubectl delete daemonset --all -n $NS --force --grace-period=0 2>/dev/null || true
              kubectl delete job --all -n $NS --force --grace-period=0 2>/dev/null || true
              kubectl delete configmap --all -n $NS --ignore-not-found=true 2>/dev/null || true
              kubectl delete secret --all -n $NS --ignore-not-found=true 2>/dev/null || true
              kubectl delete pvc --all -n $NS --ignore-not-found=true 2>/dev/null || true
            fi
          done

      - name: Delete Namespaces with Finalizer removal
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=== Namespace Finalizer ì¼ê´„ ì œê±° ë° ì‚­ì œ ==="
          for NS in petclinic monitoring argocd external-secrets; do
            if kubectl get namespace $NS > /dev/null 2>&1; then
              echo "Deleting namespace: $NS"
              
              kubectl patch namespace $NS --type merge -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
              kubectl delete namespace $NS --force --grace-period=0 --timeout=30s 2>/dev/null || true
              
              if kubectl get namespace $NS > /dev/null 2>&1; then
                echo "Force deleting namespace: $NS"
                kubectl get namespace $NS -o json 2>/dev/null | \
                  jq '.spec.finalizers = []' | \
                  kubectl replace --raw "/api/v1/namespaces/$NS/finalize" -f - 2>/dev/null || true
              fi
            fi
          done

      # ========================================================================
      # AWS ë¦¬ì†ŒìŠ¤ ì§ì ‘ ì •ë¦¬ (ìµœì¢… í™•ì¸)
      # ========================================================================
      - name: Cleanup Remaining AWS ALB/Target Groups
        run: |
          echo "=== ë‚¨ì€ ALB ìµœì¢… í™•ì¸ ë° ì‚­ì œ ==="
          aws elbv2 describe-load-balancers --query 'LoadBalancers[*].[LoadBalancerArn,LoadBalancerName]' --output text 2>/dev/null | \
          while IFS=$'\t' read -r ARN NAME; do
            if [ -n "$ARN" ] && [ "$ARN" != "None" ]; then
              if [[ "$NAME" == *petclinic* ]] || [[ "$NAME" == *argocd* ]] || [[ "$NAME" == k8s-* ]] || [[ "$NAME" == *monitoring* ]]; then
                echo "Deleting remaining ALB: $NAME"
                aws elbv2 delete-load-balancer --load-balancer-arn "$ARN" 2>/dev/null || true
              fi
            fi
          done
          
          echo "=== ìµœì¢… ëŒ€ê¸° (30ì´ˆ) ==="
          sleep 30

      - name: Summary
        run: |
          echo "âœ… Pre-cleanup ì™„ë£Œ!"
          echo ""
          echo "ì‚­ì œëœ í•­ëª©:"
          echo "  - ArgoCD Applications, ApplicationSets, AppProjects & CRDs (with finalizers)"
          echo "  - External Secrets & CRDs (with finalizers)"
          echo "  - Karpenter NodeClaims, NodePools, EC2NodeClasses & CRDs (with finalizers)"
          echo "  - Karpenterê°€ í”„ë¡œë¹„ì €ë‹í•œ EC2 ì¸ìŠ¤í„´ìŠ¤"
          echo "  - Ingresses (with finalizers)"
          echo "  - ALBs (argocd-alb, petclinic-microservices-alb, petclinic-monitoring-alb)"
          echo "  - Target Groups (k8s-* ê´€ë ¨)"
          echo "  - Namespaces (petclinic, monitoring, argocd, external-secrets)"

  # ============================================================================
  # Terraform Destroy
  # ============================================================================
  destroy:
    name: 'Terraform Destroy'
    needs: pre-cleanup
    runs-on: ubuntu-latest
    env:
      TERRAGRUNT_IGNORE_DEPENDENCY_ERRORS: "true"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      - name: Setup Terragrunt
        run: |
          wget -q https://github.com/gruntwork-io/terragrunt/releases/download/v${{ env.TG_VERSION }}/terragrunt_linux_amd64
          chmod +x terragrunt_linux_amd64
          sudo mv terragrunt_linux_amd64 /usr/local/bin/terragrunt

      # ========================================================================
      # ì‚­ì œ ìˆœì„œ: Bootstrap â†’ Compute â†’ Foundation (ì—­ìˆœ)
      # ========================================================================
      - name: Destroy Bootstrap
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'bootstrap'
        working-directory: ./bootstrap
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_db_password }}
        run: |
          echo "=== Bootstrap Layer ì‚­ì œ ==="
          terragrunt destroy -auto-approve -refresh=false --terragrunt-non-interactive 2>/dev/null || true

      - name: Destroy Compute
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'compute'
        working-directory: ./compute
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_db_password }}
        run: |
          echo "=== Compute Layer ì‚­ì œ ==="
          terragrunt destroy -auto-approve -refresh=false --terragrunt-non-interactive || true

      - name: Destroy Foundation
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'foundation'
        working-directory: ./foundation
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_db_password }}
        run: |
          echo "=== Foundation Layer ì‚­ì œ ==="
          terragrunt destroy -auto-approve -refresh=false --terragrunt-non-interactive || true

      - name: Verify Destruction
        run: |
          echo "=== ì‚­ì œ ê²€ì¦ ==="
          echo "EKS í´ëŸ¬ìŠ¤í„°:"
          aws eks list-clusters --query 'clusters' --output text || echo "None"
          echo ""
          echo "RDS ì¸ìŠ¤í„´ìŠ¤:"
          aws rds describe-db-instances --query 'DBInstances[*].DBInstanceIdentifier' --output text || echo "None"
          echo ""
          echo "VPC (petclinic ê´€ë ¨):"
          aws ec2 describe-vpcs --filters "Name=tag:Name,Values=*petclinic*" --query 'Vpcs[*].VpcId' --output text || echo "None"
          echo ""
          echo "ALB (ë‚¨ì€ ê²ƒ í™•ì¸):"
          aws elbv2 describe-load-balancers --query 'LoadBalancers[*].LoadBalancerName' --output text || echo "None"

      - name: Destroy Complete
        run: |
          echo "ğŸ‰ Terraform Destroy ì™„ë£Œ!"
          echo ""
          echo "ì‚­ì œëœ ë ˆì´ì–´: ${{ github.event.inputs.layer }}"
          echo ""
          echo "ë‚¨ì€ ë¦¬ì†ŒìŠ¤ê°€ ìˆë‹¤ë©´ AWS ì½˜ì†”ì—ì„œ ìˆ˜ë™ í™•ì¸ í•„ìš”"
